{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up"
      ],
      "metadata": {
        "id": "iK8t6lvcUEHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install Required Libraries\n",
        "# Run this cell ONCE, then follow the instructions below\n",
        "\n",
        "print(\"üì¶ Installing dependencies (this takes ~30 seconds)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Uninstall conflicting packages\n",
        "!pip uninstall -y numpy pandas scipy\n",
        "\n",
        "# Install compatible versions together\n",
        "!pip install -q numpy==1.26.4 pandas==2.2.2 scipy==1.13.1\n",
        "\n",
        "# Install gensim and nltk\n",
        "!pip install -q gensim==4.3.3 nltk\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Installation complete!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nüîÑ üîÑ üîÑ STOP! MANDATORY NEXT STEP üîÑ üîÑ üîÑ\")\n",
        "print(\"\\nYou MUST restart the runtime before continuing:\")\n",
        "print(\"   1. Click 'Runtime' in the menu bar above\")\n",
        "print(\"   2. Select 'Restart runtime'\")\n",
        "print(\"   3. When prompted, click 'Yes' to confirm\")\n",
        "print(\"   4. Then run the NEXT cell to import libraries\")\n",
        "print(\"\\n‚ö†Ô∏è  Do NOT skip this step or you will get errors!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "Ydlm9VujScm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3fda0b-8afa-44eb-cfdb-39a3c73b386c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing dependencies (this takes ~30 seconds)...\n",
            "============================================================\n",
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: pandas 2.2.2\n",
            "Uninstalling pandas-2.2.2:\n",
            "  Successfully uninstalled pandas-2.2.2\n",
            "Found existing installation: scipy 1.13.1\n",
            "Uninstalling scipy-1.13.1:\n",
            "  Successfully uninstalled scipy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "============================================================\n",
            "‚úÖ Installation complete!\n",
            "============================================================\n",
            "\n",
            "üîÑ üîÑ üîÑ STOP! MANDATORY NEXT STEP üîÑ üîÑ üîÑ\n",
            "\n",
            "You MUST restart the runtime before continuing:\n",
            "   1. Click 'Runtime' in the menu bar above\n",
            "   2. Select 'Restart runtime'\n",
            "   3. When prompted, click 'Yes' to confirm\n",
            "   4. Then run the NEXT cell to import libraries\n",
            "\n",
            "‚ö†Ô∏è  Do NOT skip this step or you will get errors!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Import Libraries\n",
        "# Run this cell ONLY AFTER restarting runtime\n",
        "\n",
        "print(\"üìö Importing libraries...\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ All libraries loaded and ready!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Gensim version: {gensim.__version__}\")\n",
        "print(\"\\nüéâ You're ready to proceed with the assignment!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N7w9-avSs7N",
        "outputId": "c251477d-afa5-4185-bdac-bb749f9e6989"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Importing libraries...\n",
            "============================================================\n",
            "‚úÖ All libraries loaded and ready!\n",
            "============================================================\n",
            "NumPy version: 1.26.4\n",
            "Gensim version: 4.3.3\n",
            "\n",
            "üéâ You're ready to proceed with the assignment!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EtSSAeOpSU46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29822f0c-7eb2-4308-af47-74807930e209"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dl91MqLJPn9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67bff881-1942-4969-a1bb-a1a67f50dfc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset loaded successfully!\n",
            "Dataset contains 280 items\n",
            "\n",
            "Columns available: ['review-data href', 'audience-reviews__name', 'audience-reviews__name href', 'audience-reviews__duration', 'audience-reviews__review']\n"
          ]
        }
      ],
      "source": [
        "# Load your CSV cleaned data\n",
        "df = pd.read_csv('/content/drive/MyDrive/saltburn.csv')  # Replace with your cleaned CSV filename\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"Dataset contains {len(df)} items\")\n",
        "print(f\"\\nColumns available: {df.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOPIC MODELING"
      ],
      "metadata": {
        "id": "aGBFSR4AS_3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced stopwords list for topic modeling\n",
        "stopwords = [\n",
        "    # Basic English stopwords\n",
        "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
        "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
        "    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
        "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\",\n",
        "    \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\",\n",
        "    \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\",\n",
        "    \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\",\n",
        "    \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\",\n",
        "    \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\",\n",
        "    \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\",\n",
        "    \"how\", \"all\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\",\n",
        "    \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\",\n",
        "    \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"ve\", \"ll\", \"amp\",\n",
        "    \"also\", \"would\", \"could\",\"may\", \"said\", \"say\", \"new\", \"first\", \"last\", \"long\", \"little\", \"much\",\n",
        "    \"well\", \"still\", \"even\", \"back\", \"good\", \"many\", \"make\", \"made\", \"us\", \"really\"\n",
        "]\n",
        "\n",
        "# ADD YOUR OWN DOMAIN-SPECIFIC STOPWORDS HERE\n",
        "# Examples: for restaurant reviews, add \"restaurant\", \"food\", \"place\"\n",
        "#           for book reviews, add \"book\", \"story\", \"read\"\n",
        "custom_stopwords = ['it']  # Fill in words specific to your dataset\n",
        "\n",
        "stopwords.extend(custom_stopwords)\n",
        "\n",
        "print(f\"‚úÖ Stopwords list loaded: {len(stopwords)} words to filter out\")\n",
        "print(f\"Custom stopwords added: {custom_stopwords}\")"
      ],
      "metadata": {
        "id": "5ZcXmCR6TFeT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c36423b-4994-4c43-a129-eaf6e626f845"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Stopwords list loaded: 152 words to filter out\n",
            "Custom stopwords added: ['it']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_for_topics(text):\n",
        "    \"\"\"\n",
        "    Aggressive text preprocessing for topic modeling:\n",
        "    - Lowercase\n",
        "    - Remove punctuation\n",
        "    - Remove stopwords\n",
        "    - Lemmatize (reduce to base form)\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove punctuation and split into words\n",
        "    words = re.findall(r'\\b[a-z]+\\b', text)\n",
        "\n",
        "    # Remove stopwords and short words (< 3 characters)\n",
        "    words = [word for word in words if word not in stopwords and len(word) >= 3]\n",
        "\n",
        "    # Lemmatize words (reduce to base form)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return words\n",
        "\n",
        "print(\"‚úÖ Preprocessing function ready\")"
      ],
      "metadata": {
        "id": "Y3TlyG95TLaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd0d860f-ec65-4609-c2ec-bf44189eaaec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Preprocessing function ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test preprocessing on one text\n",
        "text_column = 'audience-reviews__review'\n",
        "\n",
        "sample_text = df[text_column].iloc[0]\n",
        "processed = preprocess_for_topics(sample_text)\n",
        "\n",
        "print(\"Text Preprocessing Test:\")\n",
        "print(f\"Original: {sample_text[:150]}...\")\n",
        "print(f\"\\nProcessed words: {processed}\")\n",
        "print(f\"\\nNotice: lowercase, no punctuation, lemmatized, stopwords removed\")"
      ],
      "metadata": {
        "id": "h_hDFPwZTUTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "045ea29b-2cc4-40c6-8a48-c71525496cd5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Preprocessing Test:\n",
            "Original: A well-made, well acted melodrama that is stylish, provocative and deceptive....\n",
            "\n",
            "Processed words: ['acted', 'melodrama', 'stylish', 'provocative', 'deceptive']\n",
            "\n",
            "Notice: lowercase, no punctuation, lemmatized, stopwords removed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to entire dataset\n",
        "df['processed_for_topics'] = df[text_column].apply(preprocess_for_topics)\n",
        "\n",
        "print(\"‚úÖ Preprocessing complete!\")\n",
        "print(f\"\\nProcessed {len(df)} documents\")\n",
        "print(f\"\\nExample processed documents:\")\n",
        "for i in range(3):\n",
        "    print(f\"{i+1}. {df['processed_for_topics'].iloc[i][:10]}...\")"
      ],
      "metadata": {
        "id": "sZCqwg_NT2fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61585def-07e0-486c-c47d-fd9c3dd06743"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Preprocessing complete!\n",
            "\n",
            "Processed 280 documents\n",
            "\n",
            "Example processed documents:\n",
            "1. ['acted', 'melodrama', 'stylish', 'provocative', 'deceptive']...\n",
            "2. ['dont', 'like', 'jacob', 'elordi', 'play', 'character']...\n",
            "3. ['great', 'plot', 'cinematography', 'quite', 'disgusting', 'disturbing', 'moment', 'ruined']...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Technical Checkpoint 1: Data Preparation"
      ],
      "metadata": {
        "id": "E0Fa_h8AT7do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpoint: Verify data is ready for topic modeling\n",
        "doc_lengths = [len(doc) for doc in df['processed_for_topics']]\n",
        "avg_length = np.mean(doc_lengths)\n",
        "all_words = [word for doc in df['processed_for_topics'] for word in doc]\n",
        "vocab_size = len(set(all_words))\n",
        "\n",
        "print(\"üìä DATA PREPARATION CHECK\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Number of documents: {len(df)}\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Average document length: {avg_length:.1f} words\")\n",
        "print(f\"Shortest document: {min(doc_lengths)} words\")\n",
        "print(f\"Longest document: {max(doc_lengths)} words\")\n",
        "\n",
        "if avg_length < 10:\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: Average document length is very short. Topic modeling may struggle.\")\n",
        "if vocab_size < 100:\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: Vocabulary size is small. Consider reducing custom stopwords.\")"
      ],
      "metadata": {
        "id": "ZK8ZpR8mT9b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building Our Topic Model"
      ],
      "metadata": {
        "id": "6NAeYCjEUPmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gensim dictionary and corpus\n",
        "dictionary = corpora.Dictionary(df['processed_for_topics'])\n",
        "corpus = [dictionary.doc2bow(doc) for doc in df['processed_for_topics']]\n",
        "\n",
        "print(\"üìñ Dictionary and corpus created!\")\n",
        "print(f\"Total unique words in dictionary: {len(dictionary)}\")\n",
        "print(f\"Total documents in corpus: {len(corpus)}\")\n",
        "print(f\"\\nExample word-to-ID mappings:\")\n",
        "for i, (word_id, word) in enumerate(list(dictionary.items())[:10]):\n",
        "    print(f\"  ID {word_id}: {word}\")"
      ],
      "metadata": {
        "id": "wAzQdjN-ILwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experimenting with Number of Topics **"
      ],
      "metadata": {
        "id": "c96LDyqvITkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Try different numbers of topics\n",
        "def train_and_display_topics(corpus, dictionary, num_topics):\n",
        "    \"\"\"\n",
        "    Train an LDA model and display discovered topics\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"MODEL WITH {num_topics} TOPICS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=15,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    for idx in range(num_topics):\n",
        "        words = model.show_topic(idx, 10)\n",
        "        word_list = [word for word, prob in words]\n",
        "        print(f\"Topic {idx}: {', '.join(word_list)}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "print(\"üß™ EXPERIMENTING WITH DIFFERENT NUMBERS OF TOPICS\")\n",
        "print(\"Watch how topics change as we increase the number...\\n\")\n",
        "\n",
        "model_3 = train_and_display_topics(corpus, dictionary, 3)\n",
        "model_4 = train_and_display_topics(corpus, dictionary, 4)\n",
        "model_5 = train_and_display_topics(corpus, dictionary, 5)\n",
        "model_7 = train_and_display_topics(corpus, dictionary, 7)\n",
        "model_10 = train_and_display_topics(corpus, dictionary, 10)\n"
      ],
      "metadata": {
        "id": "W25boR7mIXfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Choose your best model**"
      ],
      "metadata": {
        "id": "d8I8vMwJI0ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train your final model with your chosen number of topics\n",
        "num_topics = 7  # Fill in your chosen number (3, 5, or 7)\n",
        "\n",
        "print(f\"ü§ñ Training final LDA model with {num_topics} topics...\\n\")\n",
        "\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=20,  # More passes for better final model\n",
        "    alpha='auto',\n",
        "    eta='auto'\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Final model training complete!\\n\")\n",
        "print(\"üéØ YOUR DISCOVERED TOPICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for idx in range(num_topics):\n",
        "    words = lda_model.show_topic(idx, 10)\n",
        "    word_list = [word for word, prob in words]\n",
        "    print(f\"\\nTopic {idx}: {', '.join(word_list)}\")\n",
        "    print(f\"Your interpretation/label: _____________________\")"
      ],
      "metadata": {
        "id": "tebeW4EDIy2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize your topics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(num_topics, 1, figsize=(12, 4*num_topics))\n",
        "\n",
        "if num_topics == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for idx in range(num_topics):\n",
        "    words_weights = lda_model.show_topic(idx, 10)\n",
        "    words = [word for word, weight in words_weights]\n",
        "    weights = [weight for word, weight in words_weights]\n",
        "\n",
        "    axes[idx].barh(range(len(words)), weights, color='skyblue')\n",
        "    axes[idx].set_yticks(range(len(words)))\n",
        "    axes[idx].set_yticklabels(words)\n",
        "    axes[idx].set_xlabel('Weight')\n",
        "    axes[idx].set_title(f'Topic {idx} - [Add your label here]')\n",
        "    axes[idx].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Topic visualizations complete!\")"
      ],
      "metadata": {
        "id": "jYf6ZcK4Jt69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dominant topic for each document\n",
        "def get_document_topics(lda_model, corpus):\n",
        "    \"\"\"\n",
        "    Get dominant topic assignment for each document\n",
        "    \"\"\"\n",
        "    topic_assignments = []\n",
        "\n",
        "    for doc in corpus:\n",
        "        topic_dist = lda_model.get_document_topics(doc)\n",
        "        if topic_dist:  # Check if not empty\n",
        "            dominant_topic = max(topic_dist, key=lambda x: x[1])\n",
        "            topic_assignments.append({\n",
        "                'topic_num': dominant_topic[0],\n",
        "                'topic_prob': round(dominant_topic[1], 3)\n",
        "            })\n",
        "        else:\n",
        "            topic_assignments.append({\n",
        "                'topic_num': -1,\n",
        "                'topic_prob': 0.0\n",
        "            })\n",
        "\n",
        "    return topic_assignments\n",
        "\n",
        "# Get topic assignments\n",
        "topic_info = get_document_topics(lda_model, corpus)\n",
        "df['dominant_topic'] = [t['topic_num'] for t in topic_info]\n",
        "df['topic_probability'] = [t['topic_prob'] for t in topic_info]\n",
        "\n",
        "print(\"‚úÖ Topic assignments complete!\")\n",
        "print(f\"\\nTopic distribution across documents:\")\n",
        "print(df['dominant_topic'].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "7Uo6JPgHgtUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample documents from each topic for validation\n",
        "print(\"üîç DOCUMENT-TOPIC VALIDATION CHECK\")\n",
        "print(\"=\" * 70)\n",
        "print(\"For each topic, read sample documents and assess if the assignment makes sense:\\n\")\n",
        "\n",
        "for topic_num in range(num_topics):\n",
        "    print(f\"\\nüìå TOPIC {topic_num}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Get top words for this topic\n",
        "    topic_words = lda_model.show_topic(topic_num, 8)\n",
        "    word_list = [word for word, prob in topic_words]\n",
        "    print(f\"Keywords: {', '.join(word_list)}\")\n",
        "\n",
        "    # Get sample documents from this topic\n",
        "    topic_docs = df[df['dominant_topic'] == topic_num]\n",
        "\n",
        "    if len(topic_docs) == 0:\n",
        "        print(\"No documents assigned to this topic.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nDocuments in this topic: {len(topic_docs)}\")\n",
        "    print(f\"\\nSample documents (read and assess if topic assignment makes sense):\\n\")\n",
        "\n",
        "    for i, (idx, row) in enumerate(topic_docs.head(3).iterrows(), 1):\n",
        "        print(f\"  {i}. {row[text_column][:150]}...\")\n",
        "        print(f\"     Probability: {row['topic_probability']:.3f}\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "iFKJk4Jug1_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Insights\n",
        "\n",
        "One of the most noticeable themes is shock and disgust, especially around Barry Keoghan‚Äôs performance and the film‚Äôs disturbing scenes (Topic 0). Many reviews describe these moments as gross or unsettling, but they also highlight them as part of what makes the movie memorable. Instead of seeing the discomfort as a flaw, audiences often seem to appreciate it as a deliberate part of the film‚Äôs power.\n",
        "Some focus on the story‚Äôs themes, like class politics and social manipulation, often comparing it to films like The Talented Mr. Ripley (Topic 1). These responses suggest that people enjoy the moral confusion in Saltburn because it gives them something to interpret and talk about, rather than providing a clear ‚Äúlesson.‚Äù\n",
        "Several topics center on specific scenes, character development, and stylistic choices (Topics 2 and 5). Viewers often express strong reactions to these aspects, even if they don‚Äôt always like them. This mix of fascination and ambivalence shows that audiences are drawn to films that are psychologically rich and unpredictable.\n",
        "People talk about the confusing morals in Saltburn as a key part of the movie‚Äôs appeal. They enjoy the strange, intense, and visually striking style even without a clear ending. And these reactions suggest that modern audiences are interested in stories that are morally complex, emotionally engaging, and intellectually stimulating, even if they don‚Äôt offer neat resolutions or traditional happy endings.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XgNxveCChABc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install Required Libraries\n",
        "# Run this cell ONCE, then follow the instructions below\n",
        "\n",
        "print(\"üì¶ Installing dependencies (this takes ~30 seconds)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Uninstall conflicting packages\n",
        "!pip uninstall -y numpy pandas scipy\n",
        "\n",
        "# Install compatible versions together\n",
        "!pip install -q numpy==1.26.4 pandas==2.2.2 scipy==1.13.1\n",
        "\n",
        "# Install gensim and nltk\n",
        "!pip install -q gensim==4.3.3 nltk\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Installation complete!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nüîÑ üîÑ üîÑ STOP! MANDATORY NEXT STEP üîÑ üîÑ üîÑ\")\n",
        "print(\"\\nYou MUST restart the runtime before continuing:\")\n",
        "print(\"   1. Click 'Runtime' in the menu bar above\")\n",
        "print(\"   2. Select 'Restart runtime'\")\n",
        "print(\"   3. When prompted, click 'Yes' to confirm\")\n",
        "print(\"   4. Then run the NEXT cell to import libraries\")\n",
        "print(\"\\n‚ö†Ô∏è  Do NOT skip this step or you will get errors!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "WpQKC5c1H2al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Import Libraries\n",
        "# Run this cell ONLY AFTER restarting runtime\n",
        "\n",
        "print(\"üìö Importing libraries...\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ All libraries loaded and ready!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Gensim version: {gensim.__version__}\")\n",
        "print(\"\\nüéâ You're ready to proceed with the assignment!\")"
      ],
      "metadata": {
        "id": "2iw3ZR4kH4IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "J-2FYOGuH4Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your CSV cleaned data\n",
        "df = pd.read_csv('/content/youtube (1).csv')  # Replace with your cleaned CSV filename\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"Dataset contains {len(df)} items\")\n",
        "print(f\"\\nColumns available: {df.columns.tolist()}\")"
      ],
      "metadata": {
        "id": "N8V-07TGH4p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling 2: Youtube Comments"
      ],
      "metadata": {
        "id": "LTzdsugOJAQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced stopwords list for topic modeling\n",
        "stopwords = [\n",
        "    # Basic English stopwords\n",
        "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
        "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
        "    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
        "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\",\n",
        "    \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\",\n",
        "    \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\",\n",
        "    \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\",\n",
        "    \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\",\n",
        "    \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\",\n",
        "    \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\",\n",
        "    \"how\", \"all\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\",\n",
        "    \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\",\n",
        "    \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"ve\", \"ll\", \"amp\",\n",
        "    \"also\", \"would\", \"could\",\"may\", \"said\", \"say\", \"new\", \"first\", \"last\", \"long\", \"little\", \"much\",\n",
        "    \"well\", \"still\", \"even\", \"back\", \"good\", \"many\", \"make\", \"made\", \"us\", \"really\"\n",
        "]\n",
        "\n",
        "# ADD YOUR OWN DOMAIN-SPECIFIC STOPWORDS HERE\n",
        "# Examples: for restaurant reviews, add \"restaurant\", \"food\", \"place\"\n",
        "#           for book reviews, add \"book\", \"story\", \"read\"\n",
        "custom_stopwords = ['it']  # Fill in words specific to your dataset\n",
        "\n",
        "stopwords.extend(custom_stopwords)\n",
        "\n",
        "print(f\"‚úÖ Stopwords list loaded: {len(stopwords)} words to filter out\")\n",
        "print(f\"Custom stopwords added: {custom_stopwords}\")"
      ],
      "metadata": {
        "id": "EHVGwLKSH5C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_for_topics(text):\n",
        "    \"\"\"\n",
        "    Aggressive text preprocessing for topic modeling:\n",
        "    - Lowercase\n",
        "    - Remove punctuation\n",
        "    - Remove stopwords\n",
        "    - Lemmatize (reduce to base form)\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove punctuation and split into words\n",
        "    words = re.findall(r'\\b[a-z]+\\b', text)\n",
        "\n",
        "    # Remove stopwords and short words (< 3 characters)\n",
        "    words = [word for word in words if word not in stopwords and len(word) >= 3]\n",
        "\n",
        "    # Lemmatize words (reduce to base form)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return words\n",
        "\n",
        "print(\"‚úÖ Preprocessing function ready\")"
      ],
      "metadata": {
        "id": "BSqdj21YJJNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test preprocessing on one text\n",
        "text_column = 'yt-core-attributed-string'\n",
        "\n",
        "sample_text = df[text_column].iloc[0]\n",
        "processed = preprocess_for_topics(sample_text)\n",
        "\n",
        "print(\"Text Preprocessing Test:\")\n",
        "print(f\"Original: {sample_text[:150]}...\")\n",
        "print(f\"\\nProcessed words: {processed}\")\n",
        "print(f\"\\nNotice: lowercase, no punctuation, lemmatized, stopwords removed\")"
      ],
      "metadata": {
        "id": "FRUmAhC1JK3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to entire dataset\n",
        "df['processed_for_topics'] = df[text_column].apply(preprocess_for_topics)\n",
        "\n",
        "print(\"‚úÖ Preprocessing complete!\")\n",
        "print(f\"\\nProcessed {len(df)} documents\")\n",
        "print(f\"\\nExample processed documents:\")\n",
        "for i in range(3):\n",
        "    print(f\"{i+1}. {df['processed_for_topics'].iloc[i][:10]}...\")"
      ],
      "metadata": {
        "id": "4DNreHrVJKvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Technical Checkpoint 2: Data Preparation"
      ],
      "metadata": {
        "id": "AxGqZ5W_JZyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpoint: Verify data is ready for topic modeling\n",
        "doc_lengths = [len(doc) for doc in df['processed_for_topics']]\n",
        "avg_length = np.mean(doc_lengths)\n",
        "all_words = [word for doc in df['processed_for_topics'] for word in doc]\n",
        "vocab_size = len(set(all_words))\n",
        "\n",
        "print(\"üìä DATA PREPARATION CHECK\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Number of documents: {len(df)}\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Average document length: {avg_length:.1f} words\")\n",
        "print(f\"Shortest document: {min(doc_lengths)} words\")\n",
        "print(f\"Longest document: {max(doc_lengths)} words\")\n",
        "\n",
        "if avg_length < 10:\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: Average document length is very short. Topic modeling may struggle.\")\n",
        "if vocab_size < 100:\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: Vocabulary size is small. Consider reducing custom stopwords.\")"
      ],
      "metadata": {
        "id": "1k1HnMt-KF-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building our Topic Model"
      ],
      "metadata": {
        "id": "9w0DxMTSJ4TJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gensim dictionary and corpus\n",
        "dictionary = corpora.Dictionary(df['processed_for_topics'])\n",
        "corpus = [dictionary.doc2bow(doc) for doc in df['processed_for_topics']]\n",
        "\n",
        "print(\"üìñ Dictionary and corpus created!\")\n",
        "print(f\"Total unique words in dictionary: {len(dictionary)}\")\n",
        "print(f\"Total documents in corpus: {len(corpus)}\")\n",
        "print(f\"\\nExample word-to-ID mappings:\")\n",
        "for i, (word_id, word) in enumerate(list(dictionary.items())[:10]):\n",
        "    print(f\"  ID {word_id}: {word}\")"
      ],
      "metadata": {
        "id": "-GXjnc1JJKo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experimenting with Number of Topics"
      ],
      "metadata": {
        "id": "D4_qhqZqKT2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Try different numbers of topics\n",
        "def train_and_display_topics(corpus, dictionary, num_topics):\n",
        "    \"\"\"\n",
        "    Train an LDA model and display discovered topics\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"MODEL WITH {num_topics} TOPICS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=15,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    for idx in range(num_topics):\n",
        "        words = model.show_topic(idx, 10)\n",
        "        word_list = [word for word, prob in words]\n",
        "        print(f\"Topic {idx}: {', '.join(word_list)}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "print(\"üß™ EXPERIMENTING WITH DIFFERENT NUMBERS OF TOPICS\")\n",
        "print(\"Watch how topics change as we increase the number...\\n\")\n",
        "\n",
        "model_3 = train_and_display_topics(corpus, dictionary, 3)\n",
        "model_4 = train_and_display_topics(corpus, dictionary, 4)\n",
        "model_5 = train_and_display_topics(corpus, dictionary, 5)\n",
        "model_7 = train_and_display_topics(corpus, dictionary, 7)\n",
        "model_10 = train_and_display_topics(corpus, dictionary, 10)"
      ],
      "metadata": {
        "id": "Nc1sfJNWJKhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Choose your best model**"
      ],
      "metadata": {
        "id": "YhuIT0mzKcn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train your final model with your chosen number of topics\n",
        "num_topics = 7  # Fill in your chosen number (3, 5, or 7)\n",
        "\n",
        "print(f\"ü§ñ Training final LDA model with {num_topics} topics...\\n\")\n",
        "\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=20,  # More passes for better final model\n",
        "    alpha='auto',\n",
        "    eta='auto'\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Final model training complete!\\n\")\n",
        "print(\"üéØ YOUR DISCOVERED TOPICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for idx in range(num_topics):\n",
        "    words = lda_model.show_topic(idx, 10)\n",
        "    word_list = [word for word, prob in words]\n",
        "    print(f\"\\nTopic {idx}: {', '.join(word_list)}\")\n",
        "    print(f\"Your interpretation/label: _____________________\")"
      ],
      "metadata": {
        "id": "-2lOUt3iJKOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize your topics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(num_topics, 1, figsize=(12, 4*num_topics))\n",
        "\n",
        "if num_topics == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for idx in range(num_topics):\n",
        "    words_weights = lda_model.show_topic(idx, 10)\n",
        "    words = [word for word, weight in words_weights]\n",
        "    weights = [weight for word, weight in words_weights]\n",
        "\n",
        "    axes[idx].barh(range(len(words)), weights, color='skyblue')\n",
        "    axes[idx].set_yticks(range(len(words)))\n",
        "    axes[idx].set_yticklabels(words)\n",
        "    axes[idx].set_xlabel('Weight')\n",
        "    axes[idx].set_title(f'Topic {idx} - [Add your label here]')\n",
        "    axes[idx].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Topic visualizations complete!\")"
      ],
      "metadata": {
        "id": "RfRXClSSKol1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dominant topic for each document\n",
        "def get_document_topics(lda_model, corpus):\n",
        "    \"\"\"\n",
        "    Get dominant topic assignment for each document\n",
        "    \"\"\"\n",
        "    topic_assignments = []\n",
        "\n",
        "    for doc in corpus:\n",
        "        topic_dist = lda_model.get_document_topics(doc)\n",
        "        if topic_dist:  # Check if not empty\n",
        "            dominant_topic = max(topic_dist, key=lambda x: x[1])\n",
        "            topic_assignments.append({\n",
        "                'topic_num': dominant_topic[0],\n",
        "                'topic_prob': round(dominant_topic[1], 3)\n",
        "            })\n",
        "        else:\n",
        "            topic_assignments.append({\n",
        "                'topic_num': -1,\n",
        "                'topic_prob': 0.0\n",
        "            })\n",
        "\n",
        "    return topic_assignments\n",
        "\n",
        "# Get topic assignments\n",
        "topic_info = get_document_topics(lda_model, corpus)\n",
        "df['dominant_topic'] = [t['topic_num'] for t in topic_info]\n",
        "df['topic_probability'] = [t['topic_prob'] for t in topic_info]\n",
        "\n",
        "print(\"‚úÖ Topic assignments complete!\")\n",
        "print(f\"\\nTopic distribution across documents:\")\n",
        "print(df['dominant_topic'].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "18a1NIa5Koe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample documents from each topic for validation\n",
        "print(\"üîç DOCUMENT-TOPIC VALIDATION CHECK\")\n",
        "print(\"=\" * 70)\n",
        "print(\"For each topic, read sample documents and assess if the assignment makes sense:\\n\")\n",
        "\n",
        "for topic_num in range(num_topics):\n",
        "    print(f\"\\nüìå TOPIC {topic_num}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Get top words for this topic\n",
        "    topic_words = lda_model.show_topic(topic_num, 8)\n",
        "    word_list = [word for word, prob in topic_words]\n",
        "    print(f\"Keywords: {', '.join(word_list)}\")\n",
        "\n",
        "    # Get sample documents from this topic\n",
        "    topic_docs = df[df['dominant_topic'] == topic_num]\n",
        "\n",
        "    if len(topic_docs) == 0:\n",
        "        print(\"No documents assigned to this topic.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nDocuments in this topic: {len(topic_docs)}\")\n",
        "    print(f\"\\nSample documents (read and assess if topic assignment makes sense):\\n\")\n",
        "\n",
        "    for i, (idx, row) in enumerate(topic_docs.head(3).iterrows(), 1):\n",
        "        print(f\"  {i}. {row[text_column][:150]}...\")\n",
        "        print(f\"     Probability: {row['topic_probability']:.3f}\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "TNz11iUbKoYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Insights\n",
        "\n",
        "Overall, the YouTube comments reveal that viewers experience Saltburn as a film that provokes intense emotional, psychological, and narrative debate, and the seven discovered topics help show how those conversations cluster. Many comments focus on the complicated relationship between Oliver and Felix (Topic 6), with viewers arguing over whether Oliver‚Äôs actions stemmed from genuine love, obsessive admiration, or calculated manipulation, making this the most dominant theme in the dataset. A substantial portion of the discussion also centers on character morality and psychology, particularly whether Oliver is best understood as a psychopath, sociopath, or narcissist, which appears in both Topic 3‚Äôs spoiler-filled analysis and Topic 5‚Äôs mixture of diagnostic labeling and praise for Barry Keoghan‚Äôs performance. Commenters frequently reinterpret the plot and question Oliver‚Äôs long-term intentions (Topic 2), debating whether his final success was the result of careful planning or opportunistic improvisation. Other viewers compare family dynamics, betrayal, and character parallels to outside narratives like Devilman Crybaby (Topic 0), suggesting that the movie resonates with broader cultural stories about ambition and destruction. Reactions to symbolism, key scenes, and directorial choices, especially the mansion setting, Venetia‚Äôs role, and the film‚Äôs more shocking visual moments are captured in Topic 4, while Topic 1 shows that many viewers mix emotional responses with film critique, commenting on themes such as loss, social ambition, and the believability of Oliver‚Äôs rise. Taken together, the comments portray Saltburn as a movie that audiences find morally unsettling, psychologically fascinating, and narratively ambiguous, encouraging viewers to dissect motives, scenes, performances, and broader themes long after the credits roll."
      ],
      "metadata": {
        "id": "abk8_6fNK0p5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "96fQJdk_OxJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}